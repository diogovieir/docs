---
title: ChunkQuery
description: Split text into chunks and filter based on similarity
---

# ChunkQuery: Text Splitting and Similarity Filtering

The **ChunkQuery** allows you to split text into chunks and filter them based on similarity to a given query. This feature is particularly useful in **Retrieval-Augmented Generation (RAG)** pipelines and other applications where only relevant sections of text are needed.

---

## Key Concepts

### Chunk Size (`chunk_size`)

- The maximum size of each chunk.
- Specifies how large each part of the text will be after splitting.
- **Example**: If `chunk_size` is `400`, the text will be split into chunks of up to 400 characters.

### Overlap (`overlap`)

- The number of characters shared between consecutive chunks.
- Helps maintain context between consecutive parts of the text, ensuring no loss of information at chunk boundaries.
- **Example**: If `overlap` is `200`, the last 200 characters of a chunk will be included at the beginning of the next chunk.

### Embeddings Model (`embedding_model`)

- Defines the model used for generating embeddings to calculate similarity.
- Options include:
  - **HuggingFace (`hf`)**: Uses the SentenceTransformer model (`all-MiniLM-L6-v2`).
  - **OpenAI (`openai`)**: Uses the OpenAI embedding model (`text-embedding-ada-002`). Requires an API key.

### Similarity Filtering

- Chunks are filtered based on their similarity to a query embedding.
- **Threshold**: Only chunks with similarity above the specified threshold are included.

---

## Practical Examples

### Example 1: Basic Chunk Splitting with Similarity Filtering

**Text**:
```plaintext
This is an example text that will be split into smaller chunks and filtered based on similarity to a query.
```

**Configuration**:
- Chunk size (`chunk_size`): `20`
- Overlap (`overlap`): `5`
- Embedding model (`embedding_model`): `"hf"`
- Query (`query`): `"example text"`
- Similarity threshold (`similarity_threshold`): `0.8`

**Code**:
```python
from pureai import ChunkQuery

chunk_query = ChunkQuery(chunk_size=20, overlap=5, embedding_model="hf")

data = [{"metadata": {"title": "Example"}, "page_content": "This is an example text that will be split into smaller chunks and filtered based on similarity to a query."}]
documents = chunk_query.process_documents(data, query="example text", similarity_threshold=0.8)

for doc in documents:
    print(doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Example'}, page_content='This is an example text ')
```

---

### Example 2: Using OpenAI Embeddings for Similarity Filtering

**Text**:
```plaintext
Chapter 1: Introduction to the topic. Chapter 2: Deep Dive into Methods. Chapter 3: Conclusion.
```

**Configuration**:
- Chunk size (`chunk_size`): `30`
- Overlap (`overlap`): `10`
- Embedding model (`embedding_model`): `"openai"`
- OpenAI API key (`openai_api_key`): `"your_openai_api_key"`
- Query (`query`): `"Deep Dive"`
- Similarity threshold (`similarity_threshold`): `0.75`

**Code**:
```python
chunk_query = ChunkQuery(chunk_size=30, overlap=10, embedding_model="openai", openai_api_key="your_openai_api_key")

data = [{"metadata": {"title": "Book"}, "page_content": "Chapter 1: Introduction to the topic. Chapter 2: Deep Dive into Methods. Chapter 3: Conclusion."}]
documents = chunk_query.process_documents(data, query="Deep Dive", similarity_threshold=0.75)

for doc in documents:
    print(doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Book'}, page_content='Chapter 2: Deep Dive into Methods. ')
```

---

### Example 3: Processing Multiple Documents in Parallel with Similarity Filtering

**Text**:
- Document 1: `"First document content. This is the initial text."`
- Document 2: `"Second document content. More information here."`

**Configuration**:
- Chunk size (`chunk_size`): `25`
- Overlap (`overlap`): `5`
- Embedding model (`embedding_model`): `"hf"`
- Query (`query`): `"document content"`
- Similarity threshold (`similarity_threshold`): `0.7`
- Maximum workers (`max_workers`): `2`

**Code**:
```python
chunk_query = ChunkQuery(chunk_size=25, overlap=5, embedding_model="hf")

data = [
    {"metadata": {"title": "Doc 1"}, "page_content": "First document content. This is the initial text."},
    {"metadata": {"title": "Doc 2"}, "page_content": "Second document content. More information here."}
]

documents = chunk_query.process_documents(data, query="document content", similarity_threshold=0.7, max_workers=2)
for doc in documents:
    print(doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Doc 1'}, page_content='First document content. ')
Document(metadata={'title': 'Doc 2'}, page_content='Second document content. ')
```

---

## Practical Applications

- **Text Pre-processing for Language Models**: Split large texts and filter only the relevant chunks to improve training efficiency.
- **Document Search and Retrieval**: Break down documents and retrieve only the most relevant parts based on a query.
- **Parallel Processing**: Use multithreading to process multiple documents concurrently, speeding up the analysis of large datasets.




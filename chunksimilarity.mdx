---
title: ChunkSimilarity
description: Split text into chunks and sort by similarity
---

# ChunkSimilarity: Text Splitting and Similarity Sorting

The **ChunkSimilarity** allows you to split text into chunks and sort them by their similarity to each other. This is particularly useful in applications like **content summarization**, **document clustering**, and **information retrieval**, where organizing content by relevance is essential.

---

## Key Concepts

### Chunk Size (`chunk_size`)

- The maximum size of each chunk.
- Specifies how large each part of the text will be after splitting.
- **Example**: If `chunk_size` is `50`, the text will be split into chunks of up to 50 characters.

### Overlap (`overlap`)

- The number of characters shared between consecutive chunks.
- Helps maintain context between consecutive parts of the text, ensuring no loss of information at chunk boundaries.
- **Example**: If `overlap` is `10`, the last 10 characters of a chunk will be included at the beginning of the next chunk.

### Embeddings Model (`embedding_model`)

- Defines the model used for generating embeddings to calculate similarity.
- Options include:
  - **HuggingFace (`hf`)**: Uses the SentenceTransformer model (`all-MiniLM-L6-v2`).
  - **OpenAI (`openai`)**: Uses the OpenAI embedding model (`text-embedding-ada-002`). Requires an API key.

### Similarity Sorting

- Chunks are sorted based on their similarity to each other.
- The **similarity matrix** is used to calculate and rank chunks based on their relevance.

---

## Practical Examples

### Example 1: Basic Chunk Splitting and Similarity Sorting

**Text**:
```plaintext
This is an example text that will be split into smaller chunks and sorted by similarity.
```

**Configuration**:
- Chunk size (`chunk_size`): `20`
- Overlap (`overlap`): `5`
- Embedding model (`embedding_model`): `"hf"`

**Code**:
```python
from pureai import ChunkSimilarity

chunk_similarity = ChunkSimilarity(chunk_size=20, overlap=5, embedding_model="hf")

data = [{"metadata": {"title": "Example"}, "page_content": "This is an example text that will be split into smaller chunks and sorted by similarity."}]
documents = chunk_similarity.process_documents(data)

for doc in documents:
    print(doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Example'}, page_content='This is an example text ')
Document(metadata={'title': 'Example'}, page_content='text that will be split ')
Document(metadata={'title': 'Example'}, page_content='split into smaller chunks ')
Document(metadata={'title': 'Example'}, page_content='chunks and sorted by similarity.')
```

---

### Example 2: Using OpenAI Embeddings for Similarity Sorting

**Text**:
```plaintext
Chapter 1: Introduction to the topic. Chapter 2: Deep Dive into Methods. Chapter 3: Conclusion.
```

**Configuration**:
- Chunk size (`chunk_size`): `30`
- Overlap (`overlap`): `10`
- Embedding model (`embedding_model`): `"openai"`
- OpenAI API key (`openai_api_key`): `"your_openai_api_key"`

**Code**:
```python
chunk_similarity = ChunkSimilarity(chunk_size=30, overlap=10, embedding_model="openai", openai_api_key="your_openai_api_key")

data = [{"metadata": {"title": "Book"}, "page_content": "Chapter 1: Introduction to the topic. Chapter 2: Deep Dive into Methods. Chapter 3: Conclusion."}]
documents = chunk_similarity.process_documents(data)

for doc in documents:
    print(doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Book'}, page_content='Chapter 2: Deep Dive into Methods. ')
Document(metadata={'title': 'Book'}, page_content='Chapter 1: Introduction to the topic. ')
Document(metadata={'title': 'Book'}, page_content='Chapter 3: Conclusion.')
```

---

### Example 3: Processing Multiple Documents in Parallel with Similarity Sorting

**Text**:
- Document 1: `"First document content. This is the initial text."`
- Document 2: `"Second document content. More information here."`

**Configuration**:
- Chunk size (`chunk_size`): `25`
- Overlap (`overlap`): `5`
- Embedding model (`embedding_model`): `"hf"`
- Maximum workers (`max_workers`): `2`

**Code**:
```python
chunk_similarity = ChunkSimilarity(chunk_size=25, overlap=5, embedding_model="hf")

data = [
    {"metadata": {"title": "Doc 1"}, "page_content": "First document content. This is the initial text."},
    {"metadata": {"title": "Doc 2"}, "page_content": "Second document content. More information here."}
]

documents = chunk_similarity.process_documents(data, max_workers=2)
for doc in documents:
    print(doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Doc 1'}, page_content='First document content. ')
Document(metadata={'title': 'Doc 2'}, page_content='Second document content. ')
Document(metadata={'title': 'Doc 1'}, page_content='This is the initial text.')
Document(metadata={'title': 'Doc 2'}, page_content='More information here.')
```

---

## Practical Applications

- **Text Pre-processing for Language Models**: Split large texts and sort chunks by similarity to provide better context to models.
- **Document Summarization**: Sort and prioritize chunks based on relevance, making summarization more efficient.
- **Content Clustering**: Use similarity sorting to group related content together for better content management.
- **Parallel Processing**: Use multithreading to process multiple documents concurrently, speeding up large-scale text analysis tasks.



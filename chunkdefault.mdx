---
title: ChunkDefault
description: Split text into chunks with overlap
---

# ChunkDefault: Text Splitting with Overlap

The **ChunkDefault**  provides a way to split large pieces of text into manageable chunks, with an overlap to maintain context between segments. This is especially useful in **Retrieval-Augmented Generation (RAG)** pipelines and other text processing tasks where preserving continuity is important.

---

## Key Concepts

### Chunk Size (`chunk_size`)

- The maximum size of each chunk.
- Specifies how large each part of the text will be after splitting.
- **Example**: If `chunk_size` is `300`, the text will be split into chunks with up to 300 characters.

### Overlap (`overlap`)

- The number of characters shared between consecutive chunks.
- Helps maintain context between consecutive parts of the text, ensuring no loss of information at chunk boundaries.
- **Example**: If `overlap` is `10`, the last 10 characters of a chunk will be included at the beginning of the next chunk.

### Chunk Size and Overlap Compatibility

- The **overlap** value must be smaller than the **chunk size**.
- **Validation**: If the overlap is greater than or equal to the chunk size, an error will be raised.

---

## Practical Examples

### Example 1: Basic Chunk Splitting

**Text**:
```plaintext
This is an example text that will be split into smaller chunks with overlap.
```

**Configuration**:
- Chunk size (`chunk_size`): `20`
- Overlap (`overlap`): `5`

**Code**:
```python
from pureai import ChunkDefault

chunk_default = ChunkDefault(chunk_size=20, overlap=5)

chunks = chunk_default.split_text("This is an example text that will be split into smaller chunks with overlap.")
for chunk in chunks:
    print(chunk)
```

**Output**:
```plaintext
This is an example text 
text that will be split 
plit into smaller chunks 
chunks with overlap.
```

---

### Example 2: Processing a Single Document

**Text**:
```plaintext
Chapter 1: Introduction to the topic. Chapter 2: Deep Dive into Methods. Chapter 3: Conclusion.
```

**Configuration**:
- Chunk size (`chunk_size`): `30`
- Overlap (`overlap`): `10`

**Code**:
```python
chunk_default = ChunkDefault(chunk_size=30, overlap=10)

data = [{"metadata": {"title": "Book"}, "page_content": "Chapter 1: Introduction to the topic. Chapter 2: Deep Dive into Methods. Chapter 3: Conclusion."}]
documents = chunk_default.process_single_document(data[0])

for doc in documents:
    print(doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Book'}, page_content='Chapter 1: Introduction to the topic. ')
Document(metadata={'title': 'Book'}, page_content='topic. Chapter 2: Deep Dive into Methods. ')
Document(metadata={'title': 'Book'}, page_content='Methods. Chapter 3: Conclusion.')
```

---

### Example 3: Processing Multiple Documents in Parallel

**Text**:
- Document 1: `"First document content. This is the initial text."`
- Document 2: `"Second document content. More information here."`

**Configuration**:
- Chunk size (`chunk_size`): `25`
- Overlap (`overlap`): `5`
- Maximum workers (`max_workers`): `2`

**Code**:
```python
chunk_default = ChunkDefault(chunk_size=25, overlap=5)

data = [
    {"metadata": {"title": "Doc 1"}, "page_content": "First document content. This is the initial text."},
    {"metadata": {"title": "Doc 2"}, "page_content": "Second document content. More information here."}
]

documents = chunk_default.process_documents(data, max_workers=2)
for doc in documents:
    print(doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Doc 1'}, page_content='First document content. This ')
Document(metadata={'title': 'Doc 1'}, page_content='content. This is the initial text.')
Document(metadata={'title': 'Doc 2'}, page_content='Second document content. More ')
Document(metadata={'title': 'Doc 2'}, page_content='content. More information here.')
```

---

## Practical Applications

- **Text Pre-processing for Language Models**: Split large texts into smaller, manageable chunks to facilitate training and inference.
- **Document Segmentation**: Break down long documents into smaller sections while preserving context.
- **Parallel Processing**: Use multithreading to process multiple documents concurrently, speeding up large-scale text analysis tasks.




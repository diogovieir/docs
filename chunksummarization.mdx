---
title: ChunkSummarization
description: Split text into chunks and generate summaries using HuggingFace or OpenAI
---

# ChunkSummarization: Text Splitting and Summarization

The **ChunkSummarization** class allows you to split text into chunks and generate summaries for each chunk using either **HuggingFace** or **OpenAI** models. This is particularly useful for generating concise summaries of large documents, making content more accessible and easier to understand.

---

## Key Concepts

### Chunk Size (`chunk_size`)

- The maximum size of each chunk.
- Specifies how large each part of the text will be after splitting.
- **Example**: If `chunk_size` is `50`, the text will be split into chunks of up to 50 characters.

### Summarization Model (`summarization_model`)

- Defines the model used for generating summaries.
- Options include:
  - **HuggingFace (`hf`)**: Uses the BART model (`facebook/bart-large-cnn`) for summarization.
  - **OpenAI (`openai`)**: Uses the OpenAI API (`text-davinci-003`). Requires an API key.

### Maximum Tokens (`max_tokens`)

- Defines the maximum number of tokens that can be generated in a summary.
- Helps control the length of the generated summaries.

---

## Practical Examples

### Example 1: Basic Chunk Splitting and Summarization

**Text**:
```plaintext
This is an example text that will be split into smaller chunks and summarized using a summarization model.
```

**Configuration**:
- Chunk size (`chunk_size`): `20`
- Summarization model (`summarization_model`): `"hf"`
- Maximum tokens (`max_tokens`): `50`

**Code**:
```python
from pureai import ChunkSummarization

chunk_summarization = ChunkSummarization(chunk_size=20, summarization_model="hf", max_tokens=50)

data = [{"metadata": {"title": "Example"}, "page_content": "This is an example text that will be split into smaller chunks and summarized using a summarization model."}]
documents = chunk_summarization.process_documents(data)

for doc in documents:
    print(doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Example'}, page_content='This is an example text that summarizes...')
```

---

### Example 2: Using OpenAI for Summarization

**Text**:
```plaintext
Chapter 1: Introduction to the topic. Chapter 2: Deep Dive into Methods. Chapter 3: Conclusion.
```

**Configuration**:
- Chunk size (`chunk_size`): `30`
- Summarization model (`summarization_model`): `"openai"`
- OpenAI API key (`openai_api_key`): `"your_openai_api_key"`
- Maximum tokens (`max_tokens`): `100`

**Code**:
```python
chunk_summarization = ChunkSummarization(chunk_size=30, summarization_model="openai", openai_api_key="your_openai_api_key", max_tokens=100)

data = [{"metadata": {"title": "Book"}, "page_content": "Chapter 1: Introduction to the topic. Chapter 2: Deep Dive into Methods. Chapter 3: Conclusion."}]
documents = chunk_summarization.process_documents(data)

for doc in documents:
    print(doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Book'}, page_content='Summary of Chapter 1: Introduction to the topic...')
```

---

### Example 3: Processing Multiple Documents in Parallel with Summarization

**Text**:
- Document 1: `"First document content. This is the initial text."`
- Document 2: `"Second document content. More information here."`

**Configuration**:
- Chunk size (`chunk_size`): `25`
- Summarization model (`summarization_model`): `"hf"`
- Maximum tokens (`max_tokens`): `50`
- Maximum workers (`max_workers`): `2`

**Code**:
```python
chunk_summarization = ChunkSummarization(chunk_size=25, summarization_model="hf", max_tokens=50)

data = [
    {"metadata": {"title": "Doc 1"}, "page_content": "First document content. This is the initial text."},
    {"metadata": {"title": "Doc 2"}, "page_content": "Second document content. More information here."}
]

documents = chunk_summarization.process_documents(data, max_workers=2)
for doc in documents:
    print(doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Doc 1'}, page_content='Summary of First document content...')
Document(metadata={'title': 'Doc 2'}, page_content='Summary of Second document content...')
```

---

## Practical Applications

- **Document Summarization**: Generate concise summaries for large documents, making them easier to read and understand.
- **Content Digest Creation**: Create short digests of large texts for newsletters or executive summaries.
- **Parallel Processing**: Use multithreading to process multiple documents concurrently, speeding up the summarization of large datasets.



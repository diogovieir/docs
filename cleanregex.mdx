---
title: ContentCleaner
description: Clean document content using regex patterns
---

# ContentCleaner: Text Cleaning with Regex

The **ContentCleaner** class allows you to clean the `page_content` of documents using customizable regex patterns. This feature is particularly useful for preprocessing text data by removing unwanted characters, extra whitespace, or other artifacts before further processing.

---

## Key Concepts

### Default Patterns (`default_patterns`)

- A list of standard regex patterns for content cleaning.

### Custom Patterns (`custom_patterns`)

- Additional regex patterns can be provided by the user to customize the cleaning process.
- These patterns are combined with the default patterns for more specific content cleaning needs.

---

## Practical Examples

### Example 1: Basic Content Cleaning

**Text**:
```plaintext
This is an example text!   With    extra spaces and non-ASCII characters: â, ê.
```

**Configuration**:
- Default patterns (`default_patterns`):
  - Remove extra spaces
  - Remove non-ASCII characters
  - Remove symbols at the beginning or end of lines

**Code**:
```python
from pureai import ContentCleaner, Document

content_cleaner = ContentCleaner()

doc = Document(metadata={"title": "Example"}, page_content="This is an example text!   With    extra spaces and non-ASCII characters: â, ê.")
cleaned_doc = content_cleaner.process_document(doc)

print(cleaned_doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Example'}, page_content='This is an example text! With extra spaces and non-ASCII characters: , .')
```

---

### Example 2: Using Custom Cleaning Patterns

**Text**:
```plaintext
Chapter 1: Introduction!!
Chapter 2: Methods??
Conclusion...
```

**Configuration**:
- Default patterns (`default_patterns`):
  - Remove extra spaces
  - Remove non-ASCII characters
  - Remove symbols at the beginning or end of lines
- Custom patterns (`custom_patterns`):
  - Remove multiple exclamation marks
  - Remove multiple question marks

**Code**:
```python
from pureai import ContentCleaner, Document

content_cleaner = ContentCleaner()

doc = Document(metadata={"title": "Example"}, page_content="Chapter 1: Introduction!!\nChapter 2: Methods??\nConclusion...")
custom_patterns = [r'\!+', r'\?+']
cleaned_doc = content_cleaner.process_document(doc, custom_patterns=custom_patterns)

print(cleaned_doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Example'}, page_content='Chapter 1: Introduction Chapter 2: Methods Conclusion.')
```

---

### Example 3: Cleaning Multiple Documents in Parallel

**Text**:
- Document 1: `"First document content!!! Extra symbols..."`
- Document 2: `"Second document content??? More details here..."`

**Configuration**:
- Default patterns (`default_patterns`):
  - Remove extra spaces
  - Remove non-ASCII characters
  - Remove symbols at the beginning or end of lines
- Maximum workers (`max_workers`): `2`

**Code**:
```python
from pureai import ContentCleaner, Document

content_cleaner = ContentCleaner()

documents = [
    Document(metadata={"title": "Doc 1"}, page_content="First document content!!! Extra symbols..."),
    Document(metadata={"title": "Doc 2"}, page_content="Second document content??? More details here...")
]

cleaned_documents = content_cleaner.process_documents(documents, max_workers=2)
for doc in cleaned_documents:
    print(doc)
```

**Output**:
```plaintext
Document(metadata={'title': 'Doc 1'}, page_content='First document content Extra symbols')
Document(metadata={'title': 'Doc 2'}, page_content='Second document content More details here')
```

---

## Practical Applications

- **Text Pre-processing**: Clean large volumes of text data to remove unwanted characters and artifacts, making the data ready for analysis or model training.
- **Data Quality Improvement**: Enhance the quality of text by removing non-ASCII characters, unnecessary whitespace, and other undesirable elements.
- **Parallel Processing**: Use multithreading to process multiple documents concurrently, speeding up the cleaning process for large datasets.


